{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed libs\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "import pandas as pd\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from sklearn.metrics import f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the arguments, so that control over the pipeline will be convinient\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch_size', type=int, default=300)\n",
    "parser.add_argument('--train_epoch', type=int, default=20)\n",
    "parser.add_argument('--lr', type=float, default=1e-3)\n",
    "parser.add_argument('--verbose', type=int, default=10)\n",
    "parser.add_argument('--data_path', type=str, default=\"filtered_df2019.csv\") #data path\n",
    "parser.add_argument('--device', type=str, default='cuda:0')\n",
    "parser.add_argument('--latent_dim', type=int, default=64)\n",
    "parser.add_argument('--input_length', type=int, default=20)\n",
    "parser.add_argument('--class_num', type=int, default=3)\n",
    "parser.add_argument('--EQL', type=int, default=1024)   #LLM output dimension\n",
    "parser.add_argument('--eql', type=int, default=5)\n",
    "args = parser.parse_known_args()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method combines the numerical features with the categorical features\n",
    "def formator(line):\n",
    "    result = []\n",
    "    result.append(to_float(line['satv_use']))\n",
    "    result.append(to_float(line['satm_use']))\n",
    "    result.append(to_float(line['satcomp_use']))\n",
    "    result.append(to_float(line['act_eng']))\n",
    "    result.append(to_float(line['act_math']))\n",
    "    result.append(to_float(line['act_read']))\n",
    "    result.append(to_float(line['act_sci']))\n",
    "    result.append(to_float(line['act_comp']))\n",
    "    result.append(to_float(line['nc1']))\n",
    "    result.append(to_float(line['nc2']))\n",
    "    result.append(to_float(line['nc3']))\n",
    "    result.append(to_float(line['ncav']))\n",
    "    result.append(to_float(line['hs_gpa']))   \n",
    "    result.append(to_float(line['college_gpa']))\n",
    "    label = [0,0,0]\n",
    "    if line['vt_adm_dec'][:2] == 'Ad':\n",
    "        label[0] = 1\n",
    "    elif line['vt_adm_dec'][:2] == 'De':\n",
    "        label[1] = 1\n",
    "    else:\n",
    "        label[2] = 1\n",
    "    #coll(result,line)\n",
    "    #gender(result,line)\n",
    "    ethnic(result,line)\n",
    "    return result, label\n",
    "\n",
    "#below are some helper functions to process the categorical features\n",
    "def coll(result, line):\n",
    "    college = [0 for _ in range(8)]\n",
    "    strings = ['Eng', 'Col', 'Bus', 'Int', 'Lib', 'Agr', 'Arc', 'Nat']\n",
    "    begin = line['vt_coll'][:3]\n",
    "    if begin in strings:\n",
    "        idx = strings.index(begin)\n",
    "        college[idx] += 1\n",
    "    result += college\n",
    "\n",
    "def gender(result, line):\n",
    "    gend = [0,0]\n",
    "    if line['gender'][:2] == 'Ma':\n",
    "        gend[0] += 1\n",
    "    elif line['gender'][:2] == 'Fe':\n",
    "        gend[1] += 1\n",
    "    result += gend\n",
    "\n",
    "def ethnic(result, line):\n",
    "    ethn = [0 for _ in range(6)]\n",
    "    strings = ['Whi', 'Asi', 'His', 'Non', 'Bla', 'Two']\n",
    "    begin = line['ethnic/race'][:3]\n",
    "    if begin in strings:\n",
    "        idx = strings.index(begin)\n",
    "        ethn[idx] += 1\n",
    "    result += ethn\n",
    "\n",
    "def to_float(input):\n",
    "    if input == '':\n",
    "        return -1\n",
    "    else:\n",
    "        return float(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load previously saved textual embeddings\n",
    "EQ_data = pd.read_pickle(\"data2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read input data from file, and save them into a certain format\n",
    "with open(args.data_path, 'r', newline='', encoding='utf-8') as file:\n",
    "    data = []\n",
    "    label = []\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        features, decision = formator(row)\n",
    "        data.append(features)\n",
    "        label.append(decision)\n",
    "    data = np.array(data)\n",
    "    data = (data - np.min(data,axis=0))/(np.max(data,axis=0) - np.min(data,axis=0) + 1e-6)\n",
    "    label = np.array(label,dtype=float)\n",
    "\n",
    "EQ1_array = np.array(EQ_data['EQ1_embeddings'].tolist())\n",
    "EQ2_array = np.array(EQ_data['EQ2_embeddings'].tolist())\n",
    "EQ3_array = np.array(EQ_data['EQ3_embeddings'].tolist())\n",
    "EQ4_array = np.array(EQ_data['EQ4_embeddings'].tolist())\n",
    "\n",
    "min_rows = min(data.shape[0], EQ1_array.shape[0], EQ2_array.shape[0], EQ3_array.shape[0], EQ4_array.shape[0])\n",
    "\n",
    "# Truncate all to the smallest size\n",
    "data = data[:min_rows, :]\n",
    "EQ1_array = EQ1_array[:min_rows, :]\n",
    "EQ2_array = EQ2_array[:min_rows, :]\n",
    "EQ3_array = EQ3_array[:min_rows, :]\n",
    "EQ4_array = EQ4_array[:min_rows, :]\n",
    "\n",
    "# Concatenate all embeddings\n",
    "data = np.concatenate([data, EQ1_array, EQ2_array, EQ3_array, EQ4_array], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label shape: torch.Size([12000, 3])\n",
      "Counters: [4000, 4000, 4000]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the lists for storing new data and labels\n",
    "new_data = []\n",
    "new_label = []\n",
    "\n",
    "# Initialize counters for each label category\n",
    "numbers = [0, 0, 0]\n",
    "\n",
    "# Iterate over the labels\n",
    "for i in range(label.shape[0]):\n",
    "    for j in range(3):\n",
    "        # Check if the current label is 1 and the counter is below 4000\n",
    "        if label[i][j] == 1 and numbers[j] < 4000:\n",
    "            numbers[j] += 1  # Increment the counter\n",
    "            new_data.append(data[i])  # Append the data\n",
    "            new_label.append(label[i])  # Append the label\n",
    "\n",
    "# Convert the collected data and labels to tensors\n",
    "data = torch.tensor(np.array(new_data))\n",
    "label = torch.tensor(np.array(new_label))\n",
    "\n",
    "# Print the resulting label shape and counters\n",
    "print(\"Label shape:\", label.shape)\n",
    "print(\"Counters:\", numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train, val, test data and labels\n",
    "idx = list(range(data.shape[0]))\n",
    "idx = np.linspace(0,data.shape[0]-1,data.shape[0],dtype=int)\n",
    "random.shuffle(idx)\n",
    "train_size = int(data.shape[0]*0.7)\n",
    "val_size = int(data.shape[0]*0.15)\n",
    "train_idx = idx[:train_size]\n",
    "val_idx = idx[train_size:train_size+val_size]\n",
    "test_idx = idx[train_size+val_size:]\n",
    "train_data = data[train_idx]\n",
    "val_data = data[val_idx]\n",
    "test_data = data[test_idx]\n",
    "train_label = label[train_idx]\n",
    "val_label = label[val_idx]\n",
    "test_label = label[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the dataset class\n",
    "class Admission_Dataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "#create dataset and data loader\n",
    "train_dataset = Admission_Dataset(train_data,train_label)\n",
    "val_dataset = Admission_Dataset(val_data,val_label)\n",
    "test_dataset = Admission_Dataset(test_data,test_label)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the training pipeline\n",
    "def train(model, args):\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #criterion = F1Score(num_classes=3, average='macro').to(args.device)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(args.train_epoch)):\n",
    "        model.train()  # Set model to training mode\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            data, label = batch\n",
    "            # Move data and labels to the same device as the model\n",
    "            device = next(model.parameters()).device\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            preds = model(data.float())\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(preds, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print loss for the batch\n",
    "            if batch_idx % args.verbose == 0:\n",
    "                _, f1, acc = test(model, 'val')\n",
    "                print(f'Train Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}, F1: {f1.item():.4f}, acc: {acc.item():.4f}')\n",
    "        \n",
    "        # Evaluate and print classification metrics for the epoch\n",
    "        class_accuracies, overall_f1, overall_acc = test(model, 'test')\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f'\\nTrain Epoch {epoch} Metrics:')\n",
    "        for i, acc in enumerate(class_accuracies):\n",
    "            print(f'Class {i} Accuracy: {acc:.4f}')\n",
    "        print(f'Overall F1 Score: {overall_f1:.4f}\\n')\n",
    "        print(f'Overall accuracy: {overall_acc:.4f}\\n')\n",
    "\n",
    "#this method can be used to validate the model or test the model, the parameter \"mode\" can switch whether to validate or test\n",
    "def test(model, mode='test'):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    if mode == 'test':\n",
    "        loader = test_loader\n",
    "    else:\n",
    "        loader = val_loader\n",
    "    with torch.no_grad():  # Disable gradient calculations\n",
    "        for batch in loader:\n",
    "            data, label = batch\n",
    "            # Move data and label to the same device as the model\n",
    "            device = next(model.parameters()).device\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            preds = model(data.float())\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            label = torch.argmax(label, dim=1)\n",
    "\n",
    "            # Collect predictions and labels\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "\n",
    "    # Calculate overall F1 score\n",
    "    overall_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    print(torch.tensor(all_labels[1]))\n",
    "    print(torch.tensor(all_preds[1]))\n",
    "    print(len(all_labels))\n",
    "    overall_acc = torch.sum(torch.tensor(all_labels) == torch.tensor(all_preds))/len(all_labels)\n",
    "    # Confusion matrix to calculate per-class accuracy\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "    \n",
    "    return class_accuracies, overall_f1, overall_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this class defines the backbone model, which is an MLP\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, latent_dim = 64, input_length=12, class_num=3, EQL=1024, eql=5):\n",
    "        super().__init__()\n",
    "        self.input_length=input_length\n",
    "        self.lin1 = nn.Linear(input_length+4*eql,latent_dim)\n",
    "        self.lin2 = nn.Linear(latent_dim,latent_dim)\n",
    "        #self.lin21 = nn.Linear(latent_dim,latent_dim)\n",
    "        self.lin3 = nn.Linear(latent_dim,class_num)\n",
    "        self.EQ = nn.Linear(EQL,eql)\n",
    "        self.EQL = EQL\n",
    "        self.eql = eql\n",
    "\n",
    "    def forward(self,x):\n",
    "        x0 = x[:,:self.input_length]\n",
    "        EQ1 = self.EQ(x[:,self.input_length:self.input_length+self.EQL])\n",
    "        EQ2 = self.EQ(x[:,self.input_length+self.EQL:self.input_length+2*self.EQL])\n",
    "        EQ3 = self.EQ(x[:,self.input_length+2*self.EQL:self.input_length+3*self.EQL])\n",
    "        EQ4 = self.EQ(x[:,self.input_length+3*self.EQL:])\n",
    "        x = torch.cat([x0,EQ1,EQ2,EQ3,EQ4],dim=-1)\n",
    "        x = self.lin1(x)\n",
    "        x = torch.nn.GELU()(x)\n",
    "        x = self.lin2(x)\n",
    "        x = torch.nn.GELU()(x)\n",
    "        ##x = self.lin21(x)\n",
    "        ##x = torch.nn.GELU()(x)\n",
    "        x = self.lin3(x)\n",
    "        return x\n",
    "    \n",
    "    def save_rep(self, x):\n",
    "        x0 = x[:,:self.input_length]\n",
    "        EQ1 = self.EQ(x[:,self.input_length:self.input_length+self.EQL])\n",
    "        EQ2 = self.EQ(x[:,self.input_length+self.EQL:self.input_length+2*self.EQL])\n",
    "        EQ3 = self.EQ(x[:,self.input_length+2*self.EQL:self.input_length+3*self.EQL])\n",
    "        EQ4 = self.EQ(x[:,self.input_length+3*self.EQL:])\n",
    "        x = torch.cat([x0,EQ1,EQ2,EQ3,EQ4],dim=-1)\n",
    "        np.save('original_rep.npy', x.detach().cpu().numpy())\n",
    "        x = self.lin1(x)\n",
    "        np.save('layer1_rep.npy', x.detach().cpu().numpy())\n",
    "        x = torch.nn.GELU()(x)\n",
    "        x = self.lin2(x)\n",
    "        np.save('layer2_rep.npy', x.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(1)\n",
      "1800\n",
      "Train Epoch 0, Batch 0, Loss: 1.1014, F1: 0.1807, acc: 0.3372\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "1800\n",
      "Train Epoch 0, Batch 10, Loss: 1.0782, F1: 0.4106, acc: 0.4706\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "1800\n",
      "Train Epoch 0, Batch 20, Loss: 1.0194, F1: 0.4305, acc: 0.5250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:00<00:05,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "\n",
      "Train Epoch 0 Metrics:\n",
      "Class 0 Accuracy: 0.6869\n",
      "Class 1 Accuracy: 0.3328\n",
      "Class 2 Accuracy: 0.5128\n",
      "Overall F1 Score: 0.5015\n",
      "\n",
      "Overall accuracy: 0.5083\n",
      "\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 1, Batch 0, Loss: 0.9733, F1: 0.5427, acc: 0.5428\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 1, Batch 10, Loss: 0.9179, F1: 0.5642, acc: 0.5650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:00<00:05,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 1, Batch 20, Loss: 0.9080, F1: 0.5388, acc: 0.5567\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "1800\n",
      "\n",
      "Train Epoch 1 Metrics:\n",
      "Class 0 Accuracy: 0.7761\n",
      "Class 1 Accuracy: 0.3441\n",
      "Class 2 Accuracy: 0.5162\n",
      "Overall F1 Score: 0.5321\n",
      "\n",
      "Overall accuracy: 0.5428\n",
      "\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 2, Batch 0, Loss: 0.8911, F1: 0.5664, acc: 0.5667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:00<00:05,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 2, Batch 10, Loss: 0.8695, F1: 0.5566, acc: 0.5728\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "1800\n",
      "Train Epoch 2, Batch 20, Loss: 0.8501, F1: 0.5642, acc: 0.5744\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "1800\n",
      "\n",
      "Train Epoch 2 Metrics:\n",
      "Class 0 Accuracy: 0.7155\n",
      "Class 1 Accuracy: 0.4394\n",
      "Class 2 Accuracy: 0.4889\n",
      "Overall F1 Score: 0.5456\n",
      "\n",
      "Overall accuracy: 0.5467\n",
      "\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 3, Batch 0, Loss: 0.8536, F1: 0.5791, acc: 0.5789\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 3, Batch 10, Loss: 0.8510, F1: 0.5790, acc: 0.5811\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "1800\n",
      "Train Epoch 3, Batch 20, Loss: 0.8426, F1: 0.5679, acc: 0.5850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:01<00:04,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(1)\n",
      "1800\n",
      "\n",
      "Train Epoch 3 Metrics:\n",
      "Class 0 Accuracy: 0.7542\n",
      "Class 1 Accuracy: 0.4346\n",
      "Class 2 Accuracy: 0.4821\n",
      "Overall F1 Score: 0.5521\n",
      "\n",
      "Overall accuracy: 0.5556\n",
      "\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 4, Batch 0, Loss: 0.8308, F1: 0.5800, acc: 0.5822\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 4, Batch 10, Loss: 0.8386, F1: 0.5868, acc: 0.5861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:01<00:04,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(0)\n",
      "1800\n",
      "Train Epoch 4, Batch 20, Loss: 0.8297, F1: 0.5796, acc: 0.5939\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "1800\n",
      "\n",
      "Train Epoch 4 Metrics:\n",
      "Class 0 Accuracy: 0.7710\n",
      "Class 1 Accuracy: 0.4394\n",
      "Class 2 Accuracy: 0.4855\n",
      "Overall F1 Score: 0.5597\n",
      "\n",
      "Overall accuracy: 0.5639\n",
      "\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 5, Batch 0, Loss: 0.8150, F1: 0.5862, acc: 0.5900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:01<00:04,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 5, Batch 10, Loss: 0.8244, F1: 0.5927, acc: 0.5906\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "1800\n",
      "Train Epoch 5, Batch 20, Loss: 0.8142, F1: 0.5847, acc: 0.5967\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "1800\n",
      "\n",
      "Train Epoch 5 Metrics:\n",
      "Class 0 Accuracy: 0.7694\n",
      "Class 1 Accuracy: 0.4637\n",
      "Class 2 Accuracy: 0.4668\n",
      "Overall F1 Score: 0.5615\n",
      "\n",
      "Overall accuracy: 0.5656\n",
      "\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "1800\n",
      "Train Epoch 6, Batch 0, Loss: 0.7950, F1: 0.5932, acc: 0.5989\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 6, Batch 10, Loss: 0.8103, F1: 0.5943, acc: 0.5922\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "1800\n",
      "Train Epoch 6, Batch 20, Loss: 0.7966, F1: 0.5957, acc: 0.6022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:02<00:03,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(1)\n",
      "1800\n",
      "\n",
      "Train Epoch 6 Metrics:\n",
      "Class 0 Accuracy: 0.7525\n",
      "Class 1 Accuracy: 0.5299\n",
      "Class 2 Accuracy: 0.4259\n",
      "Overall F1 Score: 0.5656\n",
      "\n",
      "Overall accuracy: 0.5694\n",
      "\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "1800\n",
      "Train Epoch 7, Batch 0, Loss: 0.7738, F1: 0.5877, acc: 0.5961\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "1800\n",
      "Train Epoch 7, Batch 10, Loss: 0.7954, F1: 0.5952, acc: 0.5939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [00:02<00:03,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(0)\n",
      "1800\n",
      "Train Epoch 7, Batch 20, Loss: 0.7751, F1: 0.5912, acc: 0.5939\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "1800\n",
      "\n",
      "Train Epoch 7 Metrics:\n",
      "Class 0 Accuracy: 0.7256\n",
      "Class 1 Accuracy: 0.5800\n",
      "Class 2 Accuracy: 0.3884\n",
      "Overall F1 Score: 0.5609\n",
      "\n",
      "Overall accuracy: 0.5656\n",
      "\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "1800\n",
      "Train Epoch 8, Batch 0, Loss: 0.7576, F1: 0.5887, acc: 0.5994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [00:02<00:03,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(0)\n",
      "1800\n",
      "Train Epoch 8, Batch 10, Loss: 0.7818, F1: 0.5927, acc: 0.5922\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 8, Batch 20, Loss: 0.7553, F1: 0.5929, acc: 0.5928\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "1800\n",
      "\n",
      "Train Epoch 8 Metrics:\n",
      "Class 0 Accuracy: 0.7290\n",
      "Class 1 Accuracy: 0.6187\n",
      "Class 2 Accuracy: 0.3424\n",
      "Overall F1 Score: 0.5557\n",
      "\n",
      "Overall accuracy: 0.5650\n",
      "\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "1800\n",
      "Train Epoch 9, Batch 0, Loss: 0.7427, F1: 0.5871, acc: 0.5994\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "1800\n",
      "Train Epoch 9, Batch 10, Loss: 0.7703, F1: 0.5947, acc: 0.5939\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 9, Batch 20, Loss: 0.7453, F1: 0.5987, acc: 0.5972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [00:02<00:02,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(1)\n",
      "1800\n",
      "\n",
      "Train Epoch 9 Metrics:\n",
      "Class 0 Accuracy: 0.7323\n",
      "Class 1 Accuracy: 0.6430\n",
      "Class 2 Accuracy: 0.3492\n",
      "Overall F1 Score: 0.5671\n",
      "\n",
      "Overall accuracy: 0.5767\n",
      "\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "1800\n",
      "Train Epoch 10, Batch 0, Loss: 0.7283, F1: 0.5892, acc: 0.6000\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "1800\n",
      "Train Epoch 10, Batch 10, Loss: 0.7588, F1: 0.5881, acc: 0.5883\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 10, Batch 20, Loss: 0.7368, F1: 0.5929, acc: 0.5911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:05<00:09,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(1)\n",
      "1800\n",
      "\n",
      "Train Epoch 10 Metrics:\n",
      "Class 0 Accuracy: 0.7273\n",
      "Class 1 Accuracy: 0.6446\n",
      "Class 2 Accuracy: 0.3492\n",
      "Overall F1 Score: 0.5664\n",
      "\n",
      "Overall accuracy: 0.5756\n",
      "\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "1800\n",
      "Train Epoch 11, Batch 0, Loss: 0.7129, F1: 0.5929, acc: 0.6028\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 11, Batch 10, Loss: 0.7476, F1: 0.5910, acc: 0.5922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [00:06<00:06,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 11, Batch 20, Loss: 0.7294, F1: 0.5907, acc: 0.5889\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "1800\n",
      "\n",
      "Train Epoch 11 Metrics:\n",
      "Class 0 Accuracy: 0.7205\n",
      "Class 1 Accuracy: 0.6462\n",
      "Class 2 Accuracy: 0.3441\n",
      "Overall F1 Score: 0.5627\n",
      "\n",
      "Overall accuracy: 0.5722\n",
      "\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "1800\n",
      "Train Epoch 12, Batch 0, Loss: 0.6974, F1: 0.5873, acc: 0.5967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [00:06<00:04,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 12, Batch 10, Loss: 0.7359, F1: 0.5925, acc: 0.5950\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 12, Batch 20, Loss: 0.7233, F1: 0.5934, acc: 0.5917\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "1800\n",
      "\n",
      "Train Epoch 12 Metrics:\n",
      "Class 0 Accuracy: 0.7222\n",
      "Class 1 Accuracy: 0.6494\n",
      "Class 2 Accuracy: 0.3458\n",
      "Overall F1 Score: 0.5647\n",
      "\n",
      "Overall accuracy: 0.5744\n",
      "\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "1800\n",
      "Train Epoch 13, Batch 0, Loss: 0.6827, F1: 0.5899, acc: 0.5989\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 13, Batch 10, Loss: 0.7240, F1: 0.5878, acc: 0.5906\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 13, Batch 20, Loss: 0.7183, F1: 0.5937, acc: 0.5917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [00:06<00:03,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(1)\n",
      "1800\n",
      "\n",
      "Train Epoch 13 Metrics:\n",
      "Class 0 Accuracy: 0.7172\n",
      "Class 1 Accuracy: 0.6559\n",
      "Class 2 Accuracy: 0.3543\n",
      "Overall F1 Score: 0.5686\n",
      "\n",
      "Overall accuracy: 0.5778\n",
      "\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "1800\n",
      "Train Epoch 14, Batch 0, Loss: 0.6691, F1: 0.5916, acc: 0.6006\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 14, Batch 10, Loss: 0.7124, F1: 0.5928, acc: 0.5956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [00:06<00:02,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 14, Batch 20, Loss: 0.7141, F1: 0.5928, acc: 0.5906\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "1800\n",
      "\n",
      "Train Epoch 14 Metrics:\n",
      "Class 0 Accuracy: 0.7189\n",
      "Class 1 Accuracy: 0.6543\n",
      "Class 2 Accuracy: 0.3595\n",
      "Overall F1 Score: 0.5705\n",
      "\n",
      "Overall accuracy: 0.5794\n",
      "\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 15, Batch 0, Loss: 0.6563, F1: 0.5924, acc: 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [00:07<00:01,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 15, Batch 10, Loss: 0.7029, F1: 0.5913, acc: 0.5939\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 15, Batch 20, Loss: 0.7109, F1: 0.5886, acc: 0.5861\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "1800\n",
      "\n",
      "Train Epoch 15 Metrics:\n",
      "Class 0 Accuracy: 0.7222\n",
      "Class 1 Accuracy: 0.6478\n",
      "Class 2 Accuracy: 0.3543\n",
      "Overall F1 Score: 0.5672\n",
      "\n",
      "Overall accuracy: 0.5767\n",
      "\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 16, Batch 0, Loss: 0.6446, F1: 0.5977, acc: 0.6044\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 16, Batch 10, Loss: 0.6957, F1: 0.5928, acc: 0.5950\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 16, Batch 20, Loss: 0.7085, F1: 0.5849, acc: 0.5822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [00:07<00:01,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(1)\n",
      "1800\n",
      "\n",
      "Train Epoch 16 Metrics:\n",
      "Class 0 Accuracy: 0.7306\n",
      "Class 1 Accuracy: 0.6365\n",
      "Class 2 Accuracy: 0.3492\n",
      "Overall F1 Score: 0.5644\n",
      "\n",
      "Overall accuracy: 0.5739\n",
      "\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 17, Batch 0, Loss: 0.6344, F1: 0.5944, acc: 0.6006\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 17, Batch 10, Loss: 0.6895, F1: 0.5955, acc: 0.5972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [00:07<00:00,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 17, Batch 20, Loss: 0.7068, F1: 0.5855, acc: 0.5828\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "1800\n",
      "\n",
      "Train Epoch 17 Metrics:\n",
      "Class 0 Accuracy: 0.7441\n",
      "Class 1 Accuracy: 0.6333\n",
      "Class 2 Accuracy: 0.3492\n",
      "Overall F1 Score: 0.5673\n",
      "\n",
      "Overall accuracy: 0.5772\n",
      "\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 18, Batch 0, Loss: 0.6257, F1: 0.5955, acc: 0.6017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [00:08<00:00,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 18, Batch 10, Loss: 0.6838, F1: 0.5945, acc: 0.5961\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 18, Batch 20, Loss: 0.7054, F1: 0.5869, acc: 0.5850\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "1800\n",
      "\n",
      "Train Epoch 18 Metrics:\n",
      "Class 0 Accuracy: 0.7475\n",
      "Class 1 Accuracy: 0.6349\n",
      "Class 2 Accuracy: 0.3441\n",
      "Overall F1 Score: 0.5665\n",
      "\n",
      "Overall accuracy: 0.5772\n",
      "\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 19, Batch 0, Loss: 0.6181, F1: 0.5927, acc: 0.5989\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 19, Batch 10, Loss: 0.6788, F1: 0.5942, acc: 0.5961\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "1800\n",
      "Train Epoch 19, Batch 20, Loss: 0.7036, F1: 0.5875, acc: 0.5867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:08<00:00,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(1)\n",
      "1800\n",
      "\n",
      "Train Epoch 19 Metrics:\n",
      "Class 0 Accuracy: 0.7475\n",
      "Class 1 Accuracy: 0.6317\n",
      "Class 2 Accuracy: 0.3458\n",
      "Overall F1 Score: 0.5657\n",
      "\n",
      "Overall accuracy: 0.5767\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#initialize a model, and train it, validation is done simultaneously during the training process\n",
    "model = MLP(latent_dim=args.latent_dim,\n",
    "            input_length=args.input_length,\n",
    "            class_num=args.class_num,\n",
    "            EQL=args.EQL,\n",
    "            eql=args.eql\n",
    "            ).to(args.device)\n",
    "train(model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llama)",
   "language": "python",
   "name": "llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
